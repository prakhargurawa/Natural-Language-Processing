{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLP_Lab_10_Prakhar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGwPXOoM1wS7"
      },
      "source": [
        "# ANLP Lab 10\n",
        "## Under-resourced language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQFdo4cB1wS-"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Consider the following dictionary entry from a Welsh-English dictionary\n",
        "\n",
        "\n",
        "**cymdogaeth (-au)** *nf* neighbourhood\n",
        "\n",
        "\n",
        "1. What are the 4 facts that are represented by this entry?\n",
        "    * cymdogaeth — headword/lemma\n",
        "    * (-au) — plural\n",
        "    * nf — noun, feminine\n",
        "    * neighbourhood — English translation\n",
        "2. How might you extract these facts automatically?\n",
        "3. Explain how you would use crowd-sourcing or gamification in the process of extracting data his dictionary entry?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhIrTgTyday8"
      },
      "source": [
        "## Task 2\n",
        "### Build a new resource\n",
        "\n",
        "In this task, we will build a new Welsh-Urdu dictionary using two existing bilingual dictionaries: Welsh-English and Urdu-English.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voErNf9y1Mmn",
        "outputId": "de955fa3-1d70-47a2-8ae3-5221a1be4bb2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTRySC9N3WF0",
        "outputId": "c0b5d89d-3ca6-4c48-d31d-2277f63f2117"
      },
      "source": [
        "%cd drive/MyDrive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1BKAvl8daOj"
      },
      "source": [
        "# Using cy_en and ur_fr create a new Welsh-Urdu bilingual dictionary\n",
        "\n",
        "with open(\"cy-en.txt\", \"r\", encoding='utf-8') as f:\n",
        "   cy_en = {entry.split(\"\\t\")[0].lower(): entry.split(\"\\t\")[1].lower() for entry in f.read().split(\"\\n\")}\n",
        "\n",
        "with open(\"ur-en.txt\", \"r\", encoding='utf-8') as f:\n",
        "    ur_en = dict()\n",
        "    for entry in f.read().split(\"\\n\"):\n",
        "        try:\n",
        "            ur_en[entry.split(\"\\t\")[0].lower()] = entry.split(\"\\t\")[1].lower()\n",
        "        except IndexError:\n",
        "            pass\n",
        "    \n",
        "# first, we need to inverse one of the dictionaries\n",
        "en_ur = {v: k for k, v in ur_en.items()}\n",
        "\n",
        "cy_ur = dict()\n",
        "for i in cy_en:\n",
        "    if cy_en[i] in en_ur:\n",
        "        cy_ur[i] = en_ur[cy_en[i]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re_GkGRT6xM6",
        "outputId": "9d3185c9-8edb-4ac3-b231-165260c20a3c"
      },
      "source": [
        "print(len(cy_en))\n",
        "print(len(ur_en))\n",
        "print(len(en_ur))\n",
        "print(len(cy_ur))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9936\n",
            "1609\n",
            "1258\n",
            "816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF3pHdSO6_JA",
        "outputId": "89783003-4f0e-4a7b-fa1a-4f80b8134587"
      },
      "source": [
        "print(cy_ur['gwyn'])\n",
        "print(ur_en['safed'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "safed\n",
            "white\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKzD02CTa9UJ"
      },
      "source": [
        "What are the flaws of the dictionary we built? What can we do to improve it?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzHRav291wTU"
      },
      "source": [
        "## Task 3\n",
        "### Frequency-based part-of-speech tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp6kJV6x1wTV"
      },
      "source": [
        "Let's build a simple POS-tagger that annotates each word in isolation with its most frequent tag.  The  only  calculations  that  are  required are POS-tag counts per word in the training data ([the Irish treebank](https://github.com/UniversalDependencies/UD_Irish-IDT/tree/master) from Univeral Dependencies). As soon as the occurrences are counted, the frequency tagger is ready to annotate sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBZEIpDObPOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b75f0f-b64a-49e4-be22-f974646de31a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Irish-IDT/master/ga_idt-ud-train.conllu\n",
        "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Irish-IDT/master/ga_idt-ud-test.conllu\n",
        "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Irish-IDT/master/ga_idt-ud-dev.conllu "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-23 03:14:50--  https://raw.githubusercontent.com/UniversalDependencies/UD_Irish-IDT/master/ga_idt-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6137980 (5.9M) [text/plain]\n",
            "Saving to: ‘ga_idt-ud-train.conllu.3’\n",
            "\n",
            "ga_idt-ud-train.con 100%[===================>]   5.85M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-23 03:14:50 (45.8 MB/s) - ‘ga_idt-ud-train.conllu.3’ saved [6137980/6137980]\n",
            "\n",
            "--2021-04-23 03:14:50--  https://raw.githubusercontent.com/UniversalDependencies/UD_Irish-IDT/master/ga_idt-ud-test.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 656899 (642K) [text/plain]\n",
            "Saving to: ‘ga_idt-ud-test.conllu.3’\n",
            "\n",
            "ga_idt-ud-test.conl 100%[===================>] 641.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-04-23 03:14:51 (14.3 MB/s) - ‘ga_idt-ud-test.conllu.3’ saved [656899/656899]\n",
            "\n",
            "--2021-04-23 03:14:51--  https://raw.githubusercontent.com/UniversalDependencies/UD_Irish-IDT/master/ga_idt-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 622849 (608K) [text/plain]\n",
            "Saving to: ‘ga_idt-ud-dev.conllu.3’\n",
            "\n",
            "ga_idt-ud-dev.conll 100%[===================>] 608.25K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-04-23 03:14:51 (13.8 MB/s) - ‘ga_idt-ud-dev.conllu.3’ saved [622849/622849]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxzKP2DRjpZJ",
        "outputId": "f3e42a43-548e-49bb-ff58-fc8737775d99"
      },
      "source": [
        "!pip install conllu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjwIohtbjWrD",
        "outputId": "5e68f7e4-d45b-4397-c12a-f3da1953d2a8"
      },
      "source": [
        "import conllu\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_eS-7UQjxtp"
      },
      "source": [
        "In [CoNLL-U format](https://universaldependencies.org/format.html), \n",
        "\n",
        "1. Word lines containing the annotation of a word/token in 10 fields separated by single tab characters; see below.\n",
        "2. Blank lines marking sentence boundaries.\n",
        "3. Comment lines starting with hash (#).\n",
        "4. Sentences consist of one or more word lines, and word lines contain the following fields:\n",
        "\n",
        "    * ID: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
        "    * FORM: Word form or punctuation symbol.\n",
        "    * LEMMA: Lemma or stem of word form.\n",
        "    * UPOS: Universal part-of-speech tag. [The description of tags](https://universaldependencies.org/u/pos/index.html).\n",
        "    * XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
        "    * FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
        "    * HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
        "    * DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
        "    * DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
        "    * MISC: Any other annotation.\n",
        "\n",
        "5. Fields must not be empty.\n",
        "6. Fields other than FORM, LEMMA, and MISC must not contain space characters.\n",
        "\n",
        "\n",
        "Here is an example. \n",
        "\n",
        "![](https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/img/dep-annot.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8htMTFO5mUOa"
      },
      "source": [
        "with open(\"ga_idt-ud-train.conllu\", \"r\", encoding=\"utf-8\") as f, open(\"ga_idt-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f1, open(\"ga_idt-ud-dev.conllu\", \"r\", encoding=\"utf-8\") as f2:\n",
        "    irish_sents = \"\\n\".join([f.read(), f1.read(), f2.read()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBQVRMdfneFW",
        "outputId": "0a1c892b-3a32-407d-e9b9-90481521002e"
      },
      "source": [
        "print(len(irish_sents))\n",
        "print(irish_sents[:505])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7308418\n",
            "# sent_id = 906\n",
            "# text = As lár na tubaiste is ea stadfaidh an ghrian sula rachaidh sí a luí san áigéan thiar ó lonrú anuas; solas ní bheidh ar fáil ach oiread na hoíche nó mar éiclips lán.\n",
            "1\tAs\tas\tADP\tSimp\t_\t0\troot\t_\t_\n",
            "2\tlár\tlár\tNOUN\tNoun\tCase=NomAcc|Gender=Masc|Number=Sing\t1\tnmod\t_\t_\n",
            "3\tna\tna\tDET\tArt\tCase=Gen|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\t4\tdet\t_\t_\n",
            "4\ttubaiste\ttubaiste\tNOUN\tNoun\tCase=Gen|Definite=Def|Gender=Fem|Number=Sing\t2\tnmod\t_\t_\n",
            "5\tis\tis\tAUX\tCop\tTense=Pres|VerbForm=Cop\t1\tcop\t_\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J76iLF3FpMGN"
      },
      "source": [
        "There is a python library `conllu` for parsing CoNLL-U format. The ourput is a list of sentences, and each token in a sentence is represented by an `OrderedDict`:\n",
        "\n",
        "```\n",
        "OrderedDict([('id', 1),\n",
        "             ('form', 'Перспективы'),\n",
        "             ('lemma', 'перспектива'),\n",
        "             ('upostag', 'NOUN'),\n",
        "             ('xpostag', None),\n",
        "             ('feats',\n",
        "              OrderedDict([('Animacy', 'Inan'),\n",
        "                           ('Case', 'Nom'),\n",
        "                           ('Gender', 'Fem'),\n",
        "                           ('Number', 'Plur')])),\n",
        "             ('head', 0),\n",
        "             ('deprel', 'ROOT'),\n",
        "             ('deps', [('root', 0)]),\n",
        "             ('misc', None)])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUSDc3PEmQUa",
        "outputId": "57274dd9-963e-406a-9574-674adb55e80a"
      },
      "source": [
        "sentences = conllu.parse(irish_sents)\n",
        "\n",
        "# first sentence\n",
        "s = sentences[0]\n",
        "# first token\n",
        "print(s[0])\n",
        "# first token's pos tag\n",
        "print(s[0]['upostag'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "As\n",
            "ADP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovob-vdFdwsr"
      },
      "source": [
        "Now, let's build a dictionary of POS-tag counts per word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG03UoNR1wTW"
      },
      "source": [
        "word_tag = dict()\n",
        "\n",
        "for sent in sentences:\n",
        "    for token in sent:\n",
        "        word = token['form']\n",
        "        pos = token['upostag']\n",
        "        # Count frequency of each word tag\n",
        "        if word in word_tag and pos in word_tag[word]:\n",
        "            word_tag[word] = {pos: word_tag[word][pos]+1}\n",
        "        else:\n",
        "            word_tag[word] = {pos: 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnC0a5myduk7"
      },
      "source": [
        "We are ready to tag new sentences. Please keep in mind, that we might encounter new words, which aren't in our dictionary and which we won't be able to tag because of that. At this stage, let's handle this problem by returning 'False' or some special token for unknown words, like 'UNK'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM9YxqxGqtHw",
        "outputId": "58dfb611-10aa-4e05-dc2c-8fda4926c98d"
      },
      "source": [
        "sentence = \"An tOireachtas is ainm don pharlaimint náisiúnta, agus sin é a bheirtear uirthi de ghnáth sa bhunreacht seo.\"\n",
        "sentence_tokenized = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Tag the sentence\n",
        "sentence_tags = list()\n",
        "for token in sentence_tokenized:\n",
        "    if word_tag.get(token, False):\n",
        "        sentence_tags.append(max(word_tag[token].keys(), key=(lambda k: word_tag[token][k])))\n",
        "    else:\n",
        "        sentence_tags.append(False)\n",
        "\n",
        "[(sentence_tokenized[i], sentence_tags[i]) for i in range(len(sentence.split()))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('An', 'DET'),\n",
              " ('tOireachtas', 'NOUN'),\n",
              " ('is', 'CCONJ'),\n",
              " ('ainm', 'NOUN'),\n",
              " ('don', 'ADP'),\n",
              " ('pharlaimint', False),\n",
              " ('náisiúnta', 'ADJ'),\n",
              " (',', 'PUNCT'),\n",
              " ('agus', 'CCONJ'),\n",
              " ('sin', 'DET'),\n",
              " ('é', 'PRON'),\n",
              " ('a', 'PART'),\n",
              " ('bheirtear', 'VERB'),\n",
              " ('uirthi', 'ADP'),\n",
              " ('de', 'ADP'),\n",
              " ('ghnáth', 'NOUN'),\n",
              " ('sa', 'ADP'),\n",
              " ('bhunreacht', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBGOWfYO1wTh"
      },
      "source": [
        "## Task 4\n",
        "### Improve the tagger\n",
        "In the previous section, we created a frequency-based pos-tagger for Irish. As you notice, some of the words could not be tagged, such as `bhunreacht` and `pharlaimint`. What would be possible solutions to this problem?\n",
        "\n",
        "Here are some ideas:\n",
        "\n",
        "1. Normalise your data before tagging. This may include:\n",
        "    * orthography standartisation (relevant for many minority languages that don't have a single spelling standard)\n",
        "    * removing initial mutations (for Celtic languages)\n",
        "    * lemmatisation (if your goal is only POS-tagging, not full morphological analysis)\n",
        "\n",
        "2. Use aligned parallel data and the induction technique (lecture slides 23-25). The goal in this case will be to use an existing POS tagger for English to annotate the English side of a parallel corpus, then project the POS-tags to the second language (Irish). Where can we get parallel corpora?\n",
        "    * https://data.europa.eu/data/datasets/\n",
        "    * https://www.clarin.eu/resource-families/parallel-corpora\n",
        "    * Datasets available on Kaggle\n",
        "\n",
        "3. Use a more sophisticated architecture to train a model that will be able to assign POS-tags to unknown words. POS-tagging is essentially a classification problem, so you can experiment with: \n",
        "    * feature engineering \n",
        "    * different classifiers using `sklearn` and `keras`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx8SKqyx3HPA"
      },
      "source": [
        "### Building a Multilayer Perceptron POS-tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aANjdGNjhKA"
      },
      "source": [
        "def reformat_data(raw_conllu_data):\n",
        "    \"\"\"\n",
        "    Returns list of sentences, where\n",
        "    each sentence is a (word, tag) tuple\n",
        "    \"\"\"\n",
        "    parsed = conllu.parse(raw_conllu_data)\n",
        "    ref_data = []\n",
        "    for sent in parsed:\n",
        "        ref_sent = []\n",
        "        for token in sent:\n",
        "            ref_sent.append((token['form'], token['upostag']))\n",
        "        ref_data.append(ref_sent)\n",
        "    return ref_data\n",
        "\n",
        "\n",
        "def add_basic_features(sentence_terms, index):\n",
        "    \"\"\" \n",
        "    Compute some very basic word features.        \n",
        "    :param sentence_terms: [w1, w2, ...] \n",
        "    :param index: the index of the word \n",
        "    :return: dict containing features\n",
        "    \"\"\"\n",
        "    term = sentence_terms[index]\n",
        "    feature_dict = {\n",
        "        'nb_terms': len(sentence_terms),\n",
        "        'term': term,\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence_terms) - 1,\n",
        "        'is_capitalized': term[0].upper() == term[0],\n",
        "        'is_all_caps': term.upper() == term,\n",
        "        'is_all_lower': term.lower() == term,\n",
        "        'prefix-1': term[0],\n",
        "        'prefix-2': term[:2],\n",
        "        'prefix-3': term[:3],\n",
        "        'suffix-1': term[-1],\n",
        "        'suffix-2': term[-2:],\n",
        "        'suffix-3': term[-3:],\n",
        "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
        "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
        "        }\n",
        "    return feature_dict\n",
        "\n",
        "\n",
        "def untag(tagged_sentence):\n",
        "    \"\"\" \n",
        "    Remove the tag for each tagged term.\n",
        "    :param tagged_sentence: a POS tagged sentence\n",
        "    :return: a list of tags\n",
        "    \"\"\"\n",
        "    return [w for w, _ in tagged_sentence]\n",
        "    \n",
        "def transform_to_dataset(tagged_sentences):\n",
        "    \"\"\"\n",
        "    Split tagged sentences to X and y datasets and append some basic features.\n",
        "    :param tagged_sentences: a list of POS tagged sentences\n",
        "    :param tagged_sentences: list of list of tuples (term_i, tag_i)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for pos_tags in tagged_sentences:\n",
        "        for index, (term, class_) in enumerate(pos_tags):\n",
        "            # Add basic NLP features for each sentence term\n",
        "            X.append(add_basic_features(untag(pos_tags), index))\n",
        "            y.append(class_)\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-rscyuzn7TQ"
      },
      "source": [
        "# Load data\n",
        "with open(\"ga_idt-ud-train.conllu\", \"r\", encoding=\"utf-8\") as f, open(\"ga_idt-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f1, open(\"ga_idt-ud-dev.conllu\", \"r\", encoding=\"utf-8\") as f2:\n",
        "    raw_train = f.read()\n",
        "    raw_test = f1.read()\n",
        "    raw_val = f2.read()\n",
        "\n",
        "# Extract words and their POS-tags from CoNLL-U format\n",
        "training_sentences = reformat_data(raw_train)\n",
        "testing_sentences = reformat_data(raw_test)\n",
        "validation_sentences = reformat_data(raw_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-lGnNG5o7qq",
        "outputId": "9545b1a0-a80c-42cf-8259-de02507027af"
      },
      "source": [
        "# Check the number of classes (tags)\n",
        "all_sentences = training_sentences + testing_sentences + validation_sentences\n",
        "\n",
        "tags = set([tag for sentence in all_sentences for _, tag in sentence])\n",
        "print(len(tags))\n",
        "tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADJ',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " 'AUX',\n",
              " 'CCONJ',\n",
              " 'DET',\n",
              " 'INTJ',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " 'PART',\n",
              " 'PRON',\n",
              " 'PROPN',\n",
              " 'PUNCT',\n",
              " 'SCONJ',\n",
              " 'SYM',\n",
              " 'VERB',\n",
              " 'X'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exluk0Tmnxhx"
      },
      "source": [
        "# Transform lists of sentenses to datasets\n",
        "X_train, y_train = transform_to_dataset(training_sentences)\n",
        "X_test, y_test = transform_to_dataset(testing_sentences)\n",
        "X_val, y_val = transform_to_dataset(validation_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npVMpRt1qhdv"
      },
      "source": [
        "#### Feature encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmyxek_cqeUT"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Fit our DictVectorizer with our set of features\n",
        "dict_vectorizer = DictVectorizer()\n",
        "dict_vectorizer.fit(X_train + X_test + X_val)\n",
        "\n",
        "# Convert dict features to vectors\n",
        "X_train = dict_vectorizer.transform(X_train)\n",
        "X_test = dict_vectorizer.transform(X_test)\n",
        "X_val = dict_vectorizer.transform(X_val)\n",
        "\n",
        "# Fit LabelEncoder with our list of classes\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train + y_test + y_val)\n",
        "\n",
        "# Encode class values as integers\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "\n",
        "# Convert integers to dummy variables (one hot encoded)\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_val = np_utils.to_categorical(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGduwSMRq2FF"
      },
      "source": [
        "#### Building the model\n",
        "\n",
        "* This kind of linear stack of layers can easily be made with the `Sequential` model. It will contain an input layer, a hidden layer, and an output layer.\n",
        "\n",
        "* To overcome overfitting, we'll use dropout regularization. We'll set the dropout rate to 20%, meaning that 20% of the randomly selected neurons will be ignored during training at each update cycle.\n",
        "\n",
        "* We'll use Rectified Linear Units (ReLU) activation for the hidden layers as they are the simplest non-linear activation functions available.\n",
        "\n",
        "* For multi-class classification, we may want to convert the outputs to probabilities, which can be done with the softmax function.\n",
        "\n",
        "* Finally, we'll use categorical cross-entropy loss function and Adam optimizer as they have proven well for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41O26_Njq5hS"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "def build_model(input_dim, hidden_neurons, output_dim):\n",
        "    \"\"\"\n",
        "    Construct, compile and return a Keras model which will be used to fit/predict\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(hidden_neurons, input_dim=input_dim),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(hidden_neurons),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "        ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXBNSLtKrkBq"
      },
      "source": [
        "#### Creating a wrapper between Keras API and Scikit-Learn\n",
        "\n",
        "Keras provides a wrapper called `KerasClassifier` which implements the Scikit-Learn classifier interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgqBCU0VrnAW"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "model_params = {\n",
        "    'build_fn': build_model,\n",
        "    'input_dim': X_train.shape[1],\n",
        "    'hidden_neurons': 512,\n",
        "    'output_dim': y_train.shape[1],\n",
        "    'epochs': 3,\n",
        "    'batch_size': 256,\n",
        "    'verbose': 1,\n",
        "    'validation_data': (X_val, y_val),\n",
        "    'shuffle': True}\n",
        "\n",
        "clf = KerasClassifier(**model_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OelMA5Yrwv3"
      },
      "source": [
        "#### Training & evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ8GAwT_ryIi",
        "outputId": "a04fbd55-a570-458c-87d3-df347690473c"
      },
      "source": [
        "hist = clf.fit(X_train, y_train)\n",
        "score = clf.score(X_test, y_test)\n",
        "\n",
        "score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "375/375 [==============================] - 45s 118ms/step - loss: 0.8654 - accuracy: 0.7542 - val_loss: 0.2733 - val_accuracy: 0.9113\n",
            "Epoch 2/3\n",
            "375/375 [==============================] - 45s 118ms/step - loss: 0.1307 - accuracy: 0.9568 - val_loss: 0.2542 - val_accuracy: 0.9241\n",
            "Epoch 3/3\n",
            "375/375 [==============================] - 45s 119ms/step - loss: 0.0762 - accuracy: 0.9749 - val_loss: 0.2621 - val_accuracy: 0.9220\n",
            "40/40 [==============================] - 0s 6ms/step - loss: 0.2707 - accuracy: 0.9154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9154219031333923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t0GIaXrwBgO"
      },
      "source": [
        "def transform_for_tagging(sentences):\n",
        "    \"\"\"\n",
        "    Split sentences to tokens and append some basic features.\n",
        "    :param sentences: a list of sentences\n",
        "    :param sentences: list of tokenised sentences \n",
        "    \"\"\"\n",
        "    X = []\n",
        "    for sent in sentences:\n",
        "        for index, word in enumerate(sent):\n",
        "            # Add basic NLP features for each sentence term\n",
        "            X.append(add_basic_features(sent, index))\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_T48e0PsTrC",
        "outputId": "be88c8b3-ef9e-4926-8bff-a1900d8882c1"
      },
      "source": [
        "sentence = \"An tOireachtas is ainm don pharlaimint náisiúnta, agus sin é a bheirtear uirthi de ghnáth sa bhunreacht seo.\"\n",
        "sentence_tokenized = nltk.word_tokenize(sentence)\n",
        "test = transform_for_tagging([sentence_tokenized])\n",
        "test = dict_vectorizer.transform(test)\n",
        "\n",
        "preds = label_encoder.inverse_transform(clf.predict(test))\n",
        "\n",
        "[(w, t) for w, t in zip(sentence_tokenized, preds)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('An', 'DET'),\n",
              " ('tOireachtas', 'NOUN'),\n",
              " ('is', 'AUX'),\n",
              " ('ainm', 'NOUN'),\n",
              " ('don', 'ADP'),\n",
              " ('pharlaimint', 'NOUN'),\n",
              " ('náisiúnta', 'ADJ'),\n",
              " (',', 'PUNCT'),\n",
              " ('agus', 'CCONJ'),\n",
              " ('sin', 'PRON'),\n",
              " ('é', 'PRON'),\n",
              " ('a', 'PART'),\n",
              " ('bheirtear', 'VERB'),\n",
              " ('uirthi', 'ADP'),\n",
              " ('de', 'ADP'),\n",
              " ('ghnáth', 'NOUN'),\n",
              " ('sa', 'ADP'),\n",
              " ('bhunreacht', 'NOUN'),\n",
              " ('seo', 'DET'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMGTos5YsIgf"
      },
      "source": [
        "clf.model.save('mlp_tagger.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}