{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLP_Lab5_Prakhar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbgYwYyWXTDU"
      },
      "source": [
        "# ANLP - Lab 5:  Textual Similarity\n",
        "\n",
        "Today's goals:\n",
        "Learn how to calculate semantic similarity using\n",
        "- methods such as bag-of-words and tf-idf\n",
        "- various measures, such as cosine distance and word mover's distance\n",
        "- resources such as WordNet\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMrhDydxsq36"
      },
      "source": [
        "## Task 1: Create a bag-of-the-words (bow) representation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxz0-neVXb8U"
      },
      "source": [
        "A bag-of-words representation for a document just lists the number of times each word occurs in the document. \n",
        "\n",
        "![](https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/img/bow.png)\n",
        "\n",
        "*Example:* The resulting bag of words model of the following three example sentences,\n",
        "\n",
        "    \"I like to play football\"\n",
        "    \"Did you go outside to play tennis\"\n",
        "    \"John and I play tennis\"\n",
        "\n",
        "looks like this:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th></th>\n",
        "    <th>Play</th>\n",
        "    <th>Tennis</th>\n",
        "    <th>To</th>\n",
        "    <th>I</th>\n",
        "    <th>Football</th>\n",
        "    <th>Did </th>\n",
        "    <th>You</th>\n",
        "    <th>go</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Sentence 1</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Sentence 2</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Sentence 3</td>\n",
        "    <td>1</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Following these steps, create a list of bags of words:\n",
        "\n",
        "    Step 1: Tokenize the Sentences\n",
        "    Step 2: Create a Dictionary of Word Frequency\n",
        "    Step 3: Creating the Bag of Words Model\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veacenP9sq3_"
      },
      "source": [
        "We scrape the Wikipedia article on [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing). The `Beautifulsoup4` libraryis used to parse the data from Wikipedia. Furthermore, Python's regex library, `re`, is used for some preprocessing tasks on the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwL4TfVuzwrg",
        "outputId": "fcf5d966-63d3-49be-fd97-29db12276cd7"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download(\"punkt\")\r\n",
        "nltk.download(\"wordnet\")\r\n",
        "\r\n",
        "import numpy as np  \r\n",
        "import random  \r\n",
        "import string\r\n",
        "import bs4 as bs  \r\n",
        "import urllib.request  \r\n",
        "import re\r\n",
        "import heapq\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX3YdqSwsq4B"
      },
      "source": [
        "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')  \n",
        "raw_html = raw_html.read()\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:  \n",
        "    article_text += para.text\n",
        "    \n",
        "corpus = nltk.sent_tokenize(article_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x7bIFCdsq4D"
      },
      "source": [
        "Now, we iterate through each sentence in the corpus, convert the sentence to lower case, and then remove the punctuation and empty spaces from the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYbmH0jasq4F"
      },
      "source": [
        "for i in range(len(corpus )):\n",
        "    corpus[i] = corpus[i].lower()\n",
        "    corpus[i] = re.sub(r'\\W', ' ', corpus [i])\n",
        "    corpus[i] = re.sub(r'\\s+', ' ', corpus [i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6MiNZjosq4F"
      },
      "source": [
        "The next step is to tokenize the sentences in the corpus and create a dictionary that contains words and their corresponding frequencies in the corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZL7pt58sq4F"
      },
      "source": [
        "wordfreq = {}\n",
        "for sentence in corpus:\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    for token in tokens:\n",
        "        if token not in wordfreq.keys():\n",
        "            wordfreq[token] = 1\n",
        "        else:\n",
        "            wordfreq[token] += 1\n",
        "\n",
        "# Filter down the vocabulary to the 200 most frequently occurring words.\n",
        "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz1KTJIqsq4H",
        "outputId": "e2ece92a-1e06-426c-9aab-07a81da6729c"
      },
      "source": [
        "sentence_vectors = []\n",
        "for sentence in corpus:\n",
        "    sentence_tokens = nltk.word_tokenize(sentence)\n",
        "    # Your code goes here\n",
        "    sent_vec = []\n",
        "    for token in most_freq:\n",
        "        if token in sentence_tokens:\n",
        "            sent_vec.append(1)\n",
        "        else:\n",
        "            sent_vec.append(0)\n",
        "    sentence_vectors.append(sent_vec)\n",
        "\n",
        "print(pd.DataFrame(data=sentence_vectors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0    1    2    3    4    5    6    ...  193  194  195  196  197  198  199\n",
            "0     1    1    1    1    1    1    1  ...    0    0    0    0    0    0    0\n",
            "1     1    1    1    0    0    0    1  ...    1    1    1    1    0    0    0\n",
            "2     1    0    0    0    1    1    0  ...    0    0    0    0    1    1    1\n",
            "3     0    0    1    0    1    1    0  ...    0    0    0    0    0    0    0\n",
            "4     1    0    1    0    0    1    0  ...    0    0    0    0    0    0    0\n",
            "5     1    1    1    0    1    1    1  ...    0    0    0    0    0    0    0\n",
            "6     1    1    1    1    1    0    1  ...    0    0    0    0    0    0    0\n",
            "7     1    1    1    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "8     1    1    1    0    0    1    1  ...    0    0    0    0    0    0    0\n",
            "9     1    1    0    1    1    1    0  ...    0    0    0    0    0    0    0\n",
            "10    1    1    1    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "11    1    1    1    1    1    1    1  ...    0    0    0    0    0    0    0\n",
            "12    1    1    1    0    0    1    1  ...    0    0    0    0    0    0    0\n",
            "13    1    1    1    0    1    1    0  ...    0    0    0    0    0    0    0\n",
            "14    1    1    0    1    0    0    1  ...    0    0    0    0    0    0    0\n",
            "15    0    1    1    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "16    1    1    0    0    0    0    1  ...    0    0    0    0    0    0    0\n",
            "17    0    0    0    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "18    1    1    0    0    0    0    1  ...    0    0    0    0    0    0    0\n",
            "19    1    1    0    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "20    1    1    1    1    1    0    0  ...    0    0    0    0    0    0    0\n",
            "21    1    1    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "22    0    0    0    0    1    0    1  ...    0    0    0    0    0    0    0\n",
            "23    1    0    0    0    0    1    0  ...    0    0    0    0    0    0    0\n",
            "24    0    0    0    1    1    1    0  ...    0    0    0    0    0    0    0\n",
            "25    0    1    0    0    0    0    1  ...    0    0    0    0    0    0    0\n",
            "26    1    0    0    1    1    0    0  ...    0    0    0    0    0    0    0\n",
            "27    1    1    0    1    1    1    1  ...    0    0    0    0    0    0    0\n",
            "28    0    0    1    0    0    1    1  ...    0    0    0    0    0    0    0\n",
            "29    1    0    1    1    1    1    0  ...    0    0    0    0    0    0    0\n",
            "30    0    1    0    1    0    0    1  ...    0    0    0    0    0    0    0\n",
            "31    1    1    1    0    0    1    1  ...    0    0    0    0    0    0    0\n",
            "32    0    1    0    1    0    1    0  ...    0    0    0    0    0    0    0\n",
            "33    0    0    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "34    0    0    0    0    0    0    1  ...    0    0    0    0    0    0    0\n",
            "35    1    1    0    1    0    1    0  ...    0    0    0    0    0    0    0\n",
            "36    1    1    1    0    1    0    0  ...    0    0    0    0    0    0    0\n",
            "37    1    1    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "38    1    1    0    1    1    0    0  ...    0    0    0    0    0    0    0\n",
            "39    1    1    0    0    1    0    0  ...    0    0    0    0    0    0    0\n",
            "40    0    1    0    0    1    0    0  ...    0    0    0    0    0    0    0\n",
            "41    1    1    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "42    1    1    1    1    0    0    1  ...    0    0    0    0    0    0    0\n",
            "43    1    1    0    1    1    1    0  ...    0    0    0    0    0    0    0\n",
            "44    1    1    0    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "45    0    1    0    1    0    0    0  ...    0    0    0    0    0    0    0\n",
            "46    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[47 rows x 200 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfeas05Ksq4H"
      },
      "source": [
        "---\n",
        "## Task 2: Creating TF-IDF Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii0Zisczsq4I"
      },
      "source": [
        "**Tf-idf** is a statistical measure used to evaluate how important a word is to a document in a collection or corpus; the importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
        "\n",
        "#### What does it mean?\n",
        "\n",
        "1. The idea behind the TF-IDF approach is that the words that are more common in one text/sentence and less common in other texts/sentences should be given high weights.\n",
        "2. If a word occurs in all texts in a corpus it’s not characteristic to any of them, so it will get a weight of 0. That’s the case of function words like “the”.\n",
        "3. If a word occurs many times in (a) particular text(s) and doesn’t occur elsewhere in the corpus, it will get a very high tf-idf score for this/these text(s) and can be used as a distinctive feature.\n",
        "\n",
        "* **Tf** = term frequency\n",
        "* **Idf** = inverse document frequency\n",
        "* **Term** ≈ word\n",
        "* **Document** ≈ sentence/text\n",
        "\n",
        "Implement TF-IDF following these steps:\n",
        "* Once you have tokenized the sentences, the next step is to find the TF-IDF value for each word in the sentence.\n",
        "* TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
        "* IDF = log((Total number of sentences)/(Number of sentences  containing the word))\n",
        "* TF-IDF = TF * IDF\n",
        "    \n",
        "![TF-IF formula](https://radimrehurek.com/gensim/_images/math/5332752ed4984e682c6a54406ac01d5dae47d6d1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlM4smMRsq4J",
        "outputId": "6f8d6433-2734-407b-dfb4-f448d03d8653"
      },
      "source": [
        "# create the TF dictionary for each word\n",
        "word_tf_values = {}\n",
        "for token in most_freq:\n",
        "    sent_tf_vector = []\n",
        "    for document in corpus:\n",
        "        doc_freq = 0\n",
        "        for word in nltk.word_tokenize(document):\n",
        "            if token == word:\n",
        "                  doc_freq += 1\n",
        "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
        "        sent_tf_vector.append(word_tf)\n",
        "    word_tf_values[token] = sent_tf_vector\n",
        "    \n",
        "# find the IDF values for the most frequently occurring words in the corpus\n",
        "word_idf_values = {}\n",
        "for token in most_freq:\n",
        "    doc_containing_word = 0\n",
        "    for document in corpus:\n",
        "        if token in nltk.word_tokenize(document):\n",
        "            doc_containing_word += 1\n",
        "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
        "\n",
        "\n",
        "# create tf-idf\n",
        "tfidf_values = []\n",
        "# Your code goes here.\n",
        "for token in word_tf_values.keys():\n",
        "    tfidf_sentences = []\n",
        "    for tf_sentence in word_tf_values[token]:\n",
        "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
        "        tfidf_sentences.append(tf_idf_score)\n",
        "    tfidf_values.append(tfidf_sentences)\n",
        "\n",
        "tf_idf_model = np.asarray(tfidf_values)\n",
        "tf_idf_model = np.transpose(tf_idf_model)\n",
        "\n",
        "print(pd.DataFrame(data=tf_idf_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         0         1         2         3    ...       196     197     198     199\n",
            "0   0.008302  0.015118  0.058393  0.034466  ...  0.000000  0.0000  0.0000  0.0000\n",
            "1   0.061674  0.042114  0.036148  0.000000  ...  0.150333  0.0000  0.0000  0.0000\n",
            "2   0.044153  0.000000  0.000000  0.000000  ...  0.000000  0.1435  0.1435  0.1435\n",
            "3   0.000000  0.000000  0.142332  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "4   0.035976  0.000000  0.084345  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "5   0.018680  0.011338  0.014598  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "6   0.019046  0.011561  0.014884  0.013178  ...  0.000000  0.0000  0.0000  0.0000\n",
            "7   0.017988  0.016378  0.042173  0.037339  ...  0.000000  0.0000  0.0000  0.0000\n",
            "8   0.026982  0.012283  0.063259  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "9   0.033495  0.030497  0.000000  0.023176  ...  0.000000  0.0000  0.0000  0.0000\n",
            "10  0.032379  0.014740  0.037955  0.033605  ...  0.000000  0.0000  0.0000  0.0000\n",
            "11  0.011165  0.010166  0.039264  0.011588  ...  0.000000  0.0000  0.0000  0.0000\n",
            "12  0.015794  0.014380  0.018515  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "13  0.017041  0.005172  0.013318  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "14  0.023128  0.028076  0.000000  0.016002  ...  0.000000  0.0000  0.0000  0.0000\n",
            "15  0.000000  0.019653  0.050607  0.044806  ...  0.000000  0.0000  0.0000  0.0000\n",
            "16  0.019046  0.017341  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "17  0.000000  0.000000  0.000000  0.029221  ...  0.000000  0.0000  0.0000  0.0000\n",
            "18  0.017041  0.015516  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "19  0.012951  0.023584  0.000000  0.026884  ...  0.000000  0.0000  0.0000  0.0000\n",
            "20  0.022590  0.013712  0.017654  0.031260  ...  0.000000  0.0000  0.0000  0.0000\n",
            "21  0.017988  0.016378  0.042173  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "22  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "23  0.020237  0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "24  0.000000  0.000000  0.000000  0.042006  ...  0.000000  0.0000  0.0000  0.0000\n",
            "25  0.000000  0.022677  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "26  0.030837  0.000000  0.000000  0.032004  ...  0.000000  0.0000  0.0000  0.0000\n",
            "27  0.006476  0.035376  0.000000  0.026884  ...  0.000000  0.0000  0.0000  0.0000\n",
            "28  0.000000  0.000000  0.021689  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "29  0.021117  0.000000  0.016502  0.029221  ...  0.000000  0.0000  0.0000  0.0000\n",
            "30  0.000000  0.017341  0.000000  0.079070  ...  0.000000  0.0000  0.0000  0.0000\n",
            "31  0.035976  0.032756  0.042173  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "32  0.000000  0.011792  0.000000  0.026884  ...  0.000000  0.0000  0.0000  0.0000\n",
            "33  0.000000  0.000000  0.047444  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "34  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "35  0.019046  0.017341  0.000000  0.039535  ...  0.000000  0.0000  0.0000  0.0000\n",
            "36  0.016604  0.030236  0.019464  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "37  0.023128  0.042114  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "38  0.034083  0.015516  0.000000  0.035373  ...  0.000000  0.0000  0.0000  0.0000\n",
            "39  0.046255  0.021057  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "40  0.000000  0.016378  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "41  0.034083  0.031032  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "42  0.027440  0.019986  0.012866  0.011391  ...  0.000000  0.0000  0.0000  0.0000\n",
            "43  0.011165  0.015248  0.000000  0.011588  ...  0.000000  0.0000  0.0000  0.0000\n",
            "44  0.014078  0.025635  0.000000  0.029221  ...  0.000000  0.0000  0.0000  0.0000\n",
            "45  0.000000  0.017341  0.000000  0.039535  ...  0.000000  0.0000  0.0000  0.0000\n",
            "46  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.0000  0.0000\n",
            "\n",
            "[47 rows x 200 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIw9-Gb6kSBS"
      },
      "source": [
        "Both bag of words and tf-idf models are available in `sklearn.feature_extraction.text` as [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) respectively. Here are some important parameters:\n",
        "\n",
        "* analyzer = 'word', 'char' or 'char_wb' (characters within word boundaries)\n",
        "* stop_words = ar list of stopwords or None\n",
        "* ngram_range = (min_n, max_n) — a list of n-gram values to be extracted\n",
        "* max_df = 1.0 — max frequency of a term\n",
        "* min_df = 1 — min frequency of a term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBBDpnKycGuG"
      },
      "source": [
        "---\n",
        "## Task 3: Cosine similarity\n",
        "**Question**: What are the most similar sentences in your `corpus` using the cosine similarity and TF-IDF?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OPvQLcdeZLM"
      },
      "source": [
        "def cos_sim(a,b):\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    return dot_product / (norm_a * norm_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbd_JK_v0Pwc",
        "outputId": "e437d286-0c19-4e8a-e842-2db95af59ed8"
      },
      "source": [
        "similarity_matrix = []\r\n",
        "# Your code goes here\r\n",
        "for sent_num_1, sent_vec_1 in enumerate(tf_idf_model):\r\n",
        "    max_similarity, max_index = 0, 0\r\n",
        "    for sent_num_2, sent_vec_2 in enumerate(tf_idf_model):\r\n",
        "        if not np.array_equal(sent_vec_1, sent_vec_2):\r\n",
        "            if cos_sim(sent_vec_1, sent_vec_2) > max_similarity:\r\n",
        "                max_similarity = cos_sim(sent_vec_1, sent_vec_2)\r\n",
        "                max_index = sent_num_2\r\n",
        "    similarity_matrix.append((sent_num_1, max_index, max_similarity))\r\n",
        "\r\n",
        "for i in similarity_matrix:\r\n",
        "    print(corpus[i[0]])\r\n",
        "    print(corpus[i[1]])\r\n",
        "    print(i[2], end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "already in 1950 alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as a criterion of intelligence a task that involves the automated interpretation and generation of natural language but at the time not articulated as a problem separate from artificial intelligence \n",
            "0.2159668907867705\n",
            "\n",
            "the result is a computer capable of understanding the contents of documents including the contextual nuances of the language within them \n",
            "the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves \n",
            "0.14601408318066153\n",
            "\n",
            "the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves \n",
            "the result is a computer capable of understanding the contents of documents including the contextual nuances of the language within them \n",
            "0.14601408318066153\n",
            "\n",
            "challenges in natural language processing frequently involve speech recognition natural language understanding and natural language generation \n",
            "the cache language models upon which many speech recognition systems now rely are examples of such statistical models \n",
            "0.24604729919040425\n",
            "\n",
            "natural language processing has its roots in the 1950s \n",
            " 36 cognitive science is the interdisciplinary scientific study of the mind and its processes \n",
            "0.3929437339519423\n",
            "\n",
            "already in 1950 alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as a criterion of intelligence a task that involves the automated interpretation and generation of natural language but at the time not articulated as a problem separate from artificial intelligence \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.2159668907867705\n",
            "\n",
            "the premise of symbolic nlp is well summarized by john searle s chinese room experiment given a collection of rules e g a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it is confronted with \n",
            "in the early days many language processing systems were designed by symbolic methods i e the hand coding of a set of rules coupled with a dictionary lookup 12 13 such as by writing grammars or devising heuristic rules for stemming \n",
            "0.42693572494337295\n",
            "\n",
            "up to the 1980s most natural language processing systems were based on complex sets of hand written rules \n",
            "some of the earliest used machine learning algorithms such as decision trees produced systems of hard if then rules similar to existing hand written rules \n",
            "0.3875374074344548\n",
            "\n",
            "starting in the late 1980s however there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing \n",
            "more recent systems based on machine learning algorithms have many advantages over hand produced rules despite the popularity of machine learning in nlp research symbolic methods are still 2020 commonly used since the so called statistical revolution 14 15 in the late 1980s and mid 1990s much natural language processing research has relied heavily on machine learning \n",
            "0.44511596171476087\n",
            "\n",
            "this was due to both the steady increase in computational power see moore s law and the gradual lessening of the dominance of chomskyan theories of linguistics e g \n",
            " 37 cognitive linguistics is an interdisciplinary branch of linguistics combining knowledge and research from both psychology and linguistics \n",
            "0.27156793907570626\n",
            "\n",
            "transformational grammar whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine learning approach to language processing \n",
            "nevertheless approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks e g of cognitive grammar 41 functional grammar 42 construction grammar 43 computational psycholinguistics and cognitive neuroscience e g act r however with limited uptake in mainstream nlp as measured by presence on major conferences 44 of the acl \n",
            "0.2720084317695307\n",
            "\n",
            " 6 in the 2010s representation learning and deep neural network style machine learning methods became widespread in natural language processing due in part to a flurry of results showing that such techniques 7 8 can achieve state of the art results in many natural language tasks for example in language modeling 9 parsing 10 11 and many others \n",
            "many different classes of machine learning algorithms have been applied to natural language processing tasks \n",
            "0.2869463607750771\n",
            "\n",
            "in the early days many language processing systems were designed by symbolic methods i e the hand coding of a set of rules coupled with a dictionary lookup 12 13 such as by writing grammars or devising heuristic rules for stemming \n",
            "the premise of symbolic nlp is well summarized by john searle s chinese room experiment given a collection of rules e g a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it is confronted with \n",
            "0.42693572494337295\n",
            "\n",
            "more recent systems based on machine learning algorithms have many advantages over hand produced rules despite the popularity of machine learning in nlp research symbolic methods are still 2020 commonly used since the so called statistical revolution 14 15 in the late 1980s and mid 1990s much natural language processing research has relied heavily on machine learning \n",
            "starting in the late 1980s however there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing \n",
            "0.44511596171476087\n",
            "\n",
            "the machine learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora the plural form of corpus is a set of documents possibly with human or computer annotations of typical real world examples \n",
            "in the early days many language processing systems were designed by symbolic methods i e the hand coding of a set of rules coupled with a dictionary lookup 12 13 such as by writing grammars or devising heuristic rules for stemming \n",
            "0.26148355058052686\n",
            "\n",
            "many different classes of machine learning algorithms have been applied to natural language processing tasks \n",
            "more recent systems based on machine learning algorithms have many advantages over hand produced rules despite the popularity of machine learning in nlp research symbolic methods are still 2020 commonly used since the so called statistical revolution 14 15 in the late 1980s and mid 1990s much natural language processing research has relied heavily on machine learning \n",
            "0.33674695001318555\n",
            "\n",
            "these algorithms take as input a large set of features that are generated from the input data \n",
            "such models are generally more robust when given unfamiliar input especially input that contains errors as is very common for real world data and produce more reliable results when integrated into a larger system comprising multiple subtasks \n",
            "0.30789220786605964\n",
            "\n",
            "increasingly however research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to each input feature \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.8106776589381047\n",
            "\n",
            "such models have the advantage that they can express the relative certainty of many different possible answers rather than only one producing more reliable results when such a model is included as a component of a larger system \n",
            "such models are generally more robust when given unfamiliar input especially input that contains errors as is very common for real world data and produce more reliable results when integrated into a larger system comprising multiple subtasks \n",
            "0.500294885719277\n",
            "\n",
            "some of the earliest used machine learning algorithms such as decision trees produced systems of hard if then rules similar to existing hand written rules \n",
            "more recent systems based on machine learning algorithms have many advantages over hand produced rules despite the popularity of machine learning in nlp research symbolic methods are still 2020 commonly used since the so called statistical revolution 14 15 in the late 1980s and mid 1990s much natural language processing research has relied heavily on machine learning \n",
            "0.40235128536435044\n",
            "\n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "increasingly however research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to each input feature \n",
            "0.8106776589381047\n",
            "\n",
            "the cache language models upon which many speech recognition systems now rely are examples of such statistical models \n",
            "challenges in natural language processing frequently involve speech recognition natural language understanding and natural language generation \n",
            "0.24604729919040425\n",
            "\n",
            "such models are generally more robust when given unfamiliar input especially input that contains errors as is very common for real world data and produce more reliable results when integrated into a larger system comprising multiple subtasks \n",
            "such models have the advantage that they can express the relative certainty of many different possible answers rather than only one producing more reliable results when such a model is included as a component of a larger system \n",
            "0.500294885719277\n",
            "\n",
            "since the neural turn statistical methods in nlp research have been largely replaced by neural networks \n",
            "since the early 2010s 16 the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning \n",
            "0.5007424272102\n",
            "\n",
            "however they continue to be relevant for contexts in which statistical interpretability and transparency is required \n",
            "though natural language processing tasks are closely intertwined they can be subdivided into categories for convenience \n",
            "0.4184744522361001\n",
            "\n",
            "a major drawback of statistical methods is that they require elaborate feature engineering \n",
            "however they continue to be relevant for contexts in which statistical interpretability and transparency is required \n",
            "0.26841603699293787\n",
            "\n",
            "since the early 2010s 16 the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning \n",
            "since the neural turn statistical methods in nlp research have been largely replaced by neural networks \n",
            "0.5007424272102\n",
            "\n",
            "popular techniques include the use of word embeddings to capture semantic properties of words and an increase in end to end learning of a higher level task e g question answering instead of relying on a pipeline of separate intermediate tasks e g part of speech tagging and dependency parsing \n",
            "this was due to both the steady increase in computational power see moore s law and the gradual lessening of the dominance of chomskyan theories of linguistics e g \n",
            "0.23862959903037242\n",
            "\n",
            "in some areas this shift has entailed substantial changes in how nlp systems are designed such that deep neural network based approaches may be viewed as a new paradigm distinct from statistical natural language processing \n",
            " 6 in the 2010s representation learning and deep neural network style machine learning methods became widespread in natural language processing due in part to a flurry of results showing that such techniques 7 8 can achieve state of the art results in many natural language tasks for example in language modeling 9 parsing 10 11 and many others \n",
            "0.2490894914691624\n",
            "\n",
            "for instance the term neural machine translation nmt emphasizes the fact that deep learning based approaches to machine translation directly learn sequence to sequence transformations obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation smt \n",
            " 6 in the 2010s representation learning and deep neural network style machine learning methods became widespread in natural language processing due in part to a flurry of results showing that such techniques 7 8 can achieve state of the art results in many natural language tasks for example in language modeling 9 parsing 10 11 and many others \n",
            "0.22758298142884661\n",
            "\n",
            "latest works tend to use non technical structure of a given task to build proper neural network \n",
            "a coarse division is given below \n",
            "0.3285866824124849\n",
            "\n",
            " 17 the following is a list of some of the most commonly researched tasks in natural language processing \n",
            "some of these tasks have direct real world applications while others more commonly serve as subtasks that are used to aid in solving larger tasks \n",
            "0.3572800143529697\n",
            "\n",
            "some of these tasks have direct real world applications while others more commonly serve as subtasks that are used to aid in solving larger tasks \n",
            " 17 the following is a list of some of the most commonly researched tasks in natural language processing \n",
            "0.3572800143529697\n",
            "\n",
            "though natural language processing tasks are closely intertwined they can be subdivided into categories for convenience \n",
            "however they continue to be relevant for contexts in which statistical interpretability and transparency is required \n",
            "0.4184744522361001\n",
            "\n",
            "a coarse division is given below \n",
            "latest works tend to use non technical structure of a given task to build proper neural network \n",
            "0.3285866824124849\n",
            "\n",
            "based on long standing trends in the field it is possible to extrapolate future directions of nlp \n",
            "as of 2020 three trends among the topics of the long standing series of conll shared tasks can be observed 35 most more higher level nlp applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language \n",
            "0.2818638829282579\n",
            "\n",
            "as of 2020 three trends among the topics of the long standing series of conll shared tasks can be observed 35 most more higher level nlp applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language \n",
            "more broadly speaking the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of nlp see trends among conll shared tasks above \n",
            "0.5036098516103843\n",
            "\n",
            "more broadly speaking the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of nlp see trends among conll shared tasks above \n",
            "as of 2020 three trends among the topics of the long standing series of conll shared tasks can be observed 35 most more higher level nlp applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language \n",
            "0.5036098516103843\n",
            "\n",
            "cognition refers to the mental action or process of acquiring knowledge and understanding through thought experience and the senses \n",
            " 37 cognitive linguistics is an interdisciplinary branch of linguistics combining knowledge and research from both psychology and linguistics \n",
            "0.20486528305127358\n",
            "\n",
            " 36 cognitive science is the interdisciplinary scientific study of the mind and its processes \n",
            "natural language processing has its roots in the 1950s \n",
            "0.3929437339519423\n",
            "\n",
            " 37 cognitive linguistics is an interdisciplinary branch of linguistics combining knowledge and research from both psychology and linguistics \n",
            " 36 cognitive science is the interdisciplinary scientific study of the mind and its processes \n",
            "0.29341940515972775\n",
            "\n",
            " 38 especially during the age of symbolic nlp the area of computational linguistics maintained strong ties with cognitive studies \n",
            "as an example george lakoff offers a methodology to build natural language processing nlp algorithms through the perspective of cognitive science along with the findings of cognitive linguistics 39 with two defining aspects ties with cognitive linguistics are part of the historical heritage of nlp but they have been less frequently addressed since the statistical turn during the 1990s \n",
            "0.4683439065852749\n",
            "\n",
            "as an example george lakoff offers a methodology to build natural language processing nlp algorithms through the perspective of cognitive science along with the findings of cognitive linguistics 39 with two defining aspects ties with cognitive linguistics are part of the historical heritage of nlp but they have been less frequently addressed since the statistical turn during the 1990s \n",
            " 38 especially during the age of symbolic nlp the area of computational linguistics maintained strong ties with cognitive studies \n",
            "0.4683439065852749\n",
            "\n",
            "nevertheless approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks e g of cognitive grammar 41 functional grammar 42 construction grammar 43 computational psycholinguistics and cognitive neuroscience e g act r however with limited uptake in mainstream nlp as measured by presence on major conferences 44 of the acl \n",
            "more recently ideas of cognitive nlp have been revived as an approach to achieve explainability e g under the notion of cognitive ai \n",
            "0.3786732787979051\n",
            "\n",
            "more recently ideas of cognitive nlp have been revived as an approach to achieve explainability e g under the notion of cognitive ai \n",
            " 45 likewise ideas of cognitive nlp are inherent to neural models multimodal nlp although rarely made explicit \n",
            "0.42699566239739756\n",
            "\n",
            " 45 likewise ideas of cognitive nlp are inherent to neural models multimodal nlp although rarely made explicit \n",
            "more recently ideas of cognitive nlp have been revived as an approach to achieve explainability e g under the notion of cognitive ai \n",
            "0.42699566239739756\n",
            "\n",
            " 46 \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtXja7ECsq4K"
      },
      "source": [
        "\n",
        "\"Cosine similarity on bag-of-words vectors is known to do well in practice, but it inherently cannot capture when documents say the same thing in completely different words\".\n",
        "\n",
        "Take, for example, two headlines:\n",
        "\n",
        "    Obama speaks to the media in Illinois\n",
        "    The President greets the press in Chicago\n",
        "\n",
        "These have no content words in common, so according to most bag of words--based metrics, their distance would be maximal.\n",
        "\n",
        "**Question**: How such a thing can be explained and how it can be addressed?\n",
        "\n",
        "\n",
        "\n",
        " ([source](https://vene.ro/blog/word-movers-distance-in-python.html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP3ueqCRsq4L"
      },
      "source": [
        "---\n",
        "## Task 4: Word mover's distance (WMD) for document classification\n",
        "\n",
        "WMD adapts the earth mover's distance to the space of documents: the distance between two texts is given by the total amount of \"mass\" needed to move the words from one side into the other, multiplied by the distance the words need to move. Read the original paper [here](https://mkusner.github.io/publications/WMD.pdf).\n",
        "\n",
        "![WMD](https://vene.ro/images/wmd-obama.png)\n",
        "\n",
        "In the following, we are going to download the Glove word embeddings and get the embeddings of a few words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7PVPf8meu3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22344b4-eb12-4ee4-9f30-34e64f169e8e"
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[============================================------] 88.7% 58.5/66.0MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ylHK4Ksq4M",
        "outputId": "810e2043-3ac5-4c33-9cd0-01073eed800c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "d1 = \"Obama speaks to the media in Illinois\"\n",
        "d2 = \"The President addresses the press in Chicago\"\n",
        "\n",
        "# Convert a collection of text documents to a matrix of token counts, except stop words\n",
        "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
        "print(\"Features:\",  \", \".join(vect.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: addresses, chicago, illinois, media, obama, president, press, speaks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7-NuqWsq4M"
      },
      "source": [
        "\n",
        "The two documents are completely orthogonal in terms of bag-of-words. What Cosine distance do you expect?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kovzn7lisq4M",
        "outputId": "a631ee59-a4fc-4af9-ccb8-6d13d8935195"
      },
      "source": [
        "v_1, v_2 = vect.transform([d1, d2])\n",
        "v_1 = v_1.toarray().ravel()\n",
        "v_2 = v_2.toarray().ravel()\n",
        "print(v_1, v_2)\n",
        "print(\"Cosine similarity (doc_1, doc_2) = {:.2f}\".format(cos_sim(v_1, v_2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 1 1 0 0 1] [1 1 0 0 0 1 1 0]\n",
            "Cosine similarity (doc_1, doc_2) = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLwb_ROosq4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1048934-335a-4994-d2ab-33d130a8eb88"
      },
      "source": [
        "# wmdistance is an inbuilt function in gensim\n",
        "similarity = word_vectors.wmdistance(d1.lower().split(), d2.lower().split())\n",
        "print(\"{:.4f}\".format(similarity))\n",
        "print(word_vectors.distance(\"media\", \"media\"))\n",
        "print(word_vectors.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.7885\n",
            "0.0\n",
            "0.74835527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIWtqEF5sq4N"
      },
      "source": [
        "Following Task 1, extract the sentenses of the [Computational Linguistics Wikipedia Page](https://en.wikipedia.org/wiki/Computational_linguistics) and sort the most similar sentences using word embeddings and WMD distance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDXSvGzIsq4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ad6679-7df3-4866-e1b4-54c6af8bc4b4"
      },
      "source": [
        "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Computational_linguistics')  \n",
        "raw_html = raw_html.read()\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:  \n",
        "    article_text += para.text\n",
        "    \n",
        "CL_corpus = nltk.sent_tokenize(article_text)\n",
        "\n",
        "for i in range(len(CL_corpus)):\n",
        "    CL_corpus[i] = CL_corpus[i].lower()\n",
        "    CL_corpus[i] = re.sub(r'\\W',' ',CL_corpus [i])\n",
        "    CL_corpus[i] = re.sub(r'\\s+',' ',CL_corpus [i])\n",
        "\n",
        "# Your code goes here\n",
        "similair_pairs = list()\n",
        "max_sim, max_indx = 0, []\n",
        "for cl_indx, cl_sent in enumerate(CL_corpus[0:50]):\n",
        "    for nlp_indx, nlp_sent in enumerate(corpus):\n",
        "        if 1 - word_vectors.wmdistance(nlp_sent, cl_sent) > max_sim:\n",
        "            max_sim = 1 - word_vectors.wmdistance(nlp_sent, cl_sent)\n",
        "            max_indx = [cl_indx, nlp_indx]\n",
        "    similair_pairs.append((max_indx, max_sim))\n",
        "    \n",
        "for sim_pair in similair_pairs:\n",
        "    print(CL_corpus[sim_pair[0][0]])\n",
        "    print(corpus[sim_pair[0][1]])\n",
        "    print(sim_pair[1], end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language as well as the study of appropriate computational approaches to linguistic questions \n",
            "the machine learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora the plural form of corpus is a set of documents possibly with human or computer annotations of typical real world examples \n",
            "0.42612991350233587\n",
            "\n",
            "computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language as well as the study of appropriate computational approaches to linguistic questions \n",
            "the machine learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora the plural form of corpus is a set of documents possibly with human or computer annotations of typical real world examples \n",
            "0.42612991350233587\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "traditionally computational linguistics emerged as an area of artificial intelligence performed by computer scientists who had specialized in the application of computers to the processing of a natural language \n",
            "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data \n",
            "0.5597274521975215\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "this includes classical problems such as the design of pos taggers part of speech taggers parsers for natural languages or tasks such as machine translation mt the sub division of computational linguistics dealing with having computers translate between languages \n",
            "however part of speech tagging introduced the use of hidden markov models to natural language processing and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data \n",
            "0.5601590265647047\n",
            "\n",
            "using models it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span \n",
            "the premise of symbolic nlp is well summarized by john searle s chinese room experiment given a collection of rules e g a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it is confronted with \n",
            "0.602485418014463\n",
            "\n",
            "using models it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span \n",
            "the premise of symbolic nlp is well summarized by john searle s chinese room experiment given a collection of rules e g a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it is confronted with \n",
            "0.602485418014463\n",
            "\n",
            "using models it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span \n",
            "the premise of symbolic nlp is well summarized by john searle s chinese room experiment given a collection of rules e g a chinese phrasebook with questions and matching answers the computer emulates natural language understanding or other nlp tasks by applying those rules to the data it is confronted with \n",
            "0.602485418014463\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p5me7Yi08ku"
      },
      "source": [
        "## Extra Task: Calculating WordNet Synset similarity (with an introduction to WordNet)\r\n",
        "\r\n",
        "[The WordNet](https://wordnet.princeton.edu/) is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. These are grouped into some set of cognitive synonyms, which are called synsets.\r\n",
        "\r\n",
        "You can get information about words as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unaa41eIsq4P",
        "outputId": "a44bf6f7-5344-4c01-b657-9c0071457d4f"
      },
      "source": [
        "from nltk.corpus import wordnet   #Import wordnet from the NLTK\n",
        "synset = wordnet.synsets(\"Travel\")\n",
        "print('Word and Type : ' + synset[0].name())\n",
        "print('Synonym of Travel is: ' + synset[0].lemmas()[0].name())\n",
        "print('The meaning of the word : ' + synset[0].definition())\n",
        "print('Example of Travel : ' + str(synset[0].examples()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word and Type : travel.n.01\n",
            "Synonym of Travel is: travel\n",
            "The meaning of the word : the act of going from one place to another\n",
            "Example of Travel : ['he enjoyed selling but he hated the travel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-CjpVHzsq4P"
      },
      "source": [
        "Here we will see how wordnet returns the synonyms and antonyms of a given word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MEPb29Ksq4Q",
        "outputId": "4afffc4e-779b-462e-cf77-0f124efc3b2f"
      },
      "source": [
        "syn = list()\n",
        "ant = list()\n",
        "for synset in wordnet.synsets(\"Worse\"):\n",
        "   for lemma in synset.lemmas():\n",
        "      syn.append(lemma.name())    #add the synonyms\n",
        "      if lemma.antonyms():    #When antonyms are available, add them into the list\n",
        "          ant.append(lemma.antonyms()[0].name())\n",
        "print('Synonyms: ' + str(syn))\n",
        "print('Antonyms: ' + str(ant))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Synonyms: ['worse', 'worse', 'worse', 'worsened', 'bad', 'bad', 'big', 'bad', 'tough', 'bad', 'spoiled', 'spoilt', 'regretful', 'sorry', 'bad', 'bad', 'uncollectible', 'bad', 'bad', 'bad', 'risky', 'high-risk', 'speculative', 'bad', 'unfit', 'unsound', 'bad', 'bad', 'bad', 'forged', 'bad', 'defective', 'worse']\n",
            "Antonyms: ['better', 'better', 'good', 'unregretful']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-FToKlvsq4Q",
        "outputId": "20bbb35f-0732-4daf-aa6e-ffaf59be9d85"
      },
      "source": [
        "first_word = wordnet.synset(\"Travel.v.01\")\n",
        "second_word = wordnet.synset(\"Walk.v.01\")\n",
        "print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))\n",
        "first_word = wordnet.synset(\"Good.n.01\")\n",
        "second_word = wordnet.synset(\"zebra.n.01\")\n",
        "print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WordNet similarity: 0.6666666666666666\n",
            "WordNet similarity: 0.09090909090909091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-YUVrRsq4R"
      },
      "source": [
        "---\n",
        "This session was inspired by the followings:\n",
        "- [https://stackabuse.com/python-for-nlp-creating-tf-idf-model-from-scratch/](https://stackabuse.com/python-for-nlp-creating-tf-idf-model-from-scratch/)\n",
        "- [https://vene.ro/blog/word-movers-distance-in-python.html](https://vene.ro/blog/word-movers-distance-in-python.html)\n",
        "- [https://radimrehurek.com/gensim/models/tfidfmodel.html](https://radimrehurek.com/gensim/models/tfidfmodel.html)\n",
        "- [https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    }
  ]
}