{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLP Lab 2 - Solutions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIDfql4s3_RJ"
      },
      "source": [
        "# ANLP Lab 2 - Solutions\n",
        "\n",
        "In this lab we are going to play with the pretrained GloVe embeddings and use them to solve some analogy problems.\n",
        "\n",
        "We start by loading some required modules:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dTa-rrg4UQt"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k7D-Tbv4ZkR"
      },
      "source": [
        "Then we obtain the GloVe embeddings as follows (this may take a few minutes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxKCMTiM4eVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "317fcacc-2441-40dc-c5c7-24352c58d178"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n",
        "!wget http://john.mccr.ae/downloads/glove.6B.50d.txt.gz\n",
        "!gunzip glove.6B.50d.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 10:34:59--  http://john.mccr.ae/downloads/glove.6B.50d.txt.gz\n",
            "Resolving john.mccr.ae (john.mccr.ae)... 128.199.47.101\n",
            "Connecting to john.mccr.ae (john.mccr.ae)|128.199.47.101|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69182520 (66M) [application/x-gzip]\n",
            "Saving to: ‘glove.6B.50d.txt.gz’\n",
            "\n",
            "glove.6B.50d.txt.gz 100%[===================>]  65.98M  21.7MB/s    in 3.0s    \n",
            "\n",
            "2020-10-19 10:35:02 (21.7 MB/s) - ‘glove.6B.50d.txt.gz’ saved [69182520/69182520]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNG_IrUU4kPK"
      },
      "source": [
        "We load the dataset as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bvWQwqi4qbp"
      },
      "source": [
        "path_to_glove_file = \"glove.6B.50d.txt\"\n",
        "\n",
        "embeddings = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1FnFwUVxAq"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Write a function to calculate the cosine similarity between two words using the GloVe vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A84biQhbd8X"
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def sim(word1, word2):\n",
        "  return dot(embeddings[word1],embeddings[word2])/norm(embeddings[word1])/norm(embeddings[word2])\n",
        "\n",
        "assert(sim(\"cat\",\"dog\")>0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYg9ijQgc7Dy"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Which word is closer to \"nurse\", \"man\" or \"woman\"? How about \"programmer\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6otFvnwdO-o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "168aff02-3409-4807-d9e2-ff84ece22698"
      },
      "source": [
        "print(sim(\"nurse\",\"man\"))\n",
        "print(sim(\"nurse\",\"woman\"))\n",
        "print(sim(\"programmer\",\"man\"))\n",
        "print(sim(\"programmer\",\"woman\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5718703\n",
            "0.7155021\n",
            "0.26579538\n",
            "0.2192782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ69SO_9dcVJ"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Implement word similarity using Euclidean distance. Do you get the same result for Question 2?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcYsqd3EdoM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ac0abb0b-9915-4744-cfee-0f2c73244a0a"
      },
      "source": [
        "from math import sqrt\n",
        "\n",
        "def sim_ed(word1, word2):\n",
        "  return norm(embeddings[word1]-embeddings[word2])\n",
        "\n",
        "print(sim_ed(\"nurse\",\"man\"))\n",
        "print(sim_ed(\"nurse\",\"woman\"))\n",
        "print(sim_ed(\"programmer\",\"man\"))\n",
        "print(sim_ed(\"programmer\",\"woman\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.715742\n",
            "4.0068955\n",
            "5.7321897\n",
            "6.151346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J798m1eGeqXn"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "According to the model of analogy, we would expect that $v_{queen} \\simeq v_{king} - v_{man} + v_{woman}$. Test this hypothesis do you think it holds?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7Oc4Q-e8UP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ef39787-56f8-45d7-89c5-1b0a376f84bc"
      },
      "source": [
        "def cos(v1, v2):\n",
        "  return dot(v1,v2)/norm(v1)/norm(v2)\n",
        "\n",
        "print(cos(embeddings[\"queen\"], embeddings[\"king\"] - embeddings[\"man\"] + embeddings[\"woman\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8609582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNDVTgH5flK7"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write a function that given a vector finds the words with the top 10 most similar embeddings. Using this find the words that are most similar to $v_{king} - v_{man} + v_{woman}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjBrvB81f5XJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "404a80ec-5e82-421e-8af2-58086bec6327"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def analogy(v):\n",
        "  return Counter({ word: cos(e,v) for word, e in embeddings.items() }).most_common(10)\n",
        "\n",
        "analogy(embeddings[\"king\"] - embeddings[\"man\"] + embeddings[\"woman\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8859835),\n",
              " ('queen', 0.8609582),\n",
              " ('daughter', 0.76845115),\n",
              " ('prince', 0.7640699),\n",
              " ('throne', 0.76349694),\n",
              " ('princess', 0.7512728),\n",
              " ('elizabeth', 0.75064886),\n",
              " ('father', 0.73144966),\n",
              " ('kingdom', 0.7296158),\n",
              " ('mother', 0.72800094)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a9ymcNZgn6w"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Repeat the example using the 3CosMul method as defined in the lectures. Do you get a different result?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkralSF-gu3K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "ea827771-3dbb-47c6-9fed-8a2321807309"
      },
      "source": [
        "def cos2(v1,v2):\n",
        "  return (cos(v1,v2) + 1) / 2\n",
        "  \n",
        "def three_cosmul_analogy(m,w,k):\n",
        "  return Counter({ word: cos2(v,embeddings[k]) * cos2(v,embeddings[m]) / (cos2(v,embeddings[w]) + 1e-6) for word, v in embeddings.items() }).most_common(10)\n",
        "\n",
        "three_cosmul_analogy(\"woman\",\"man\",\"king\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.9288907978427993),\n",
              " ('king', 0.9218768373440346),\n",
              " ('throne', 0.882325271864579),\n",
              " ('elizabeth', 0.8789501295328435),\n",
              " ('princess', 0.8767548497811588),\n",
              " ('daughter', 0.8705160447955236),\n",
              " ('prince', 0.8702554959921912),\n",
              " ('kingdom', 0.8607221035520414),\n",
              " ('eldest', 0.8595449106596545),\n",
              " ('monarch', 0.8584720608347555)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}