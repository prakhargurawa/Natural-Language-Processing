{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2_Prakhar_Gurawa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC-LZ20S_WUr"
      },
      "source": [
        "# Assignment 2: Information Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xqCFJBv_WUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96eb1405-ef1f-4cd0-9479-7b41589aef3e"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('all')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq8sr9x17KhU"
      },
      "source": [
        "## Task 1: Named Entity Annotation (10 Marks)\n",
        "\n",
        "Using the IOB tagging scheme annotate all of the named entities (PERson, LOCation, ORGanisation, TIME) in the following sentence:\n",
        "\n",
        "*Wayne Rooney is a professional footballer from England who last played for Major League Soccer club D.C. United and will join Derby County in January 2020.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htlSW1ad81D-"
      },
      "source": [
        "Wayne (B-PER) Rooney (I-PER) is (O) a (O) professional (O) footballer (O) from (O) England (B-LOC) who (O) last (O) played (O) for (O) Major (O) League (O) Soccer (O) club (O) D.C. (B-ORG) United (I-ORG) and (O) will (O) join (O) Derby (B-LOC) County (I-LOC) in (O) January (B-TIME) 2020 (I-TIME) . (O)  \n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZNmDWxj-V-J"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### For subsequent tasks in this assignment, you will work with the documents in `football_players.txt` to perform various information extraction tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9YE4n6u7olU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde3b093-c44f-4877-d91b-c28fb1793527"
      },
      "source": [
        "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
        "# !curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpSaij2b73Vj"
      },
      "source": [
        " Read all the documents from `football_players.txt` into a list called `docs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qteh89cs7q4x"
      },
      "source": [
        "docs = []\n",
        "# your code goes here\n",
        "paragraphs_file = open(\"drive/My Drive/datasets/football_players.txt\", encoding=\"UTF-8\")\n",
        "all_content = paragraphs_file.read() #reading all the content in one step\n",
        "#using the string methods we split it\n",
        "\n",
        "# https://stackoverflow.com/questions/53641202/how-to-read-from-text-file-into-array-paragraph-by-paragraph/53641266\n",
        "delimiter = \"\\n\"\n",
        "paragraphs = all_content.split(delimiter)\n",
        "\n",
        "for para in paragraphs:\n",
        "    if len(para)!=0: # checking for non empty paragraphs\n",
        "        docs.append(para)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCEJrJ-p_WU1"
      },
      "source": [
        "## Task 2 (10 Marks)\n",
        "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
        "\n",
        "Please keep in mind that the expected output is a list within a list as shown below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO7Fyfq7DYxW"
      },
      "source": [
        "Hint: For this task you need to perform three steps:\n",
        "1. Sentence Segmentation\n",
        "1. Word Tokenization\n",
        "1. Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3MCJIcR_WU2"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def ie_preprocess(document):\n",
        "  # your code goes here\n",
        "  sentence = sent_tokenize(document)        # tokenizing whole paragraph in different sentences\n",
        "  posTagSentences = []\n",
        "  for sent in sentence:\n",
        "      words = word_tokenize(sent)           # tokenizing each sentance into words\n",
        "      posTagSentence = nltk.pos_tag(words)  # part-of-speech taggaing\n",
        "      posTagSentences.append(posTagSentence)\n",
        "  return posTagSentences"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E04CUNb_WU6"
      },
      "source": [
        "Run the cell below to verify your result for the second sentence in the first document.\n",
        "Expected output: \n",
        "`[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R30taRgf_WU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2811fc1f-cdf5-4b82-86a1-70889822de72"
      },
      "source": [
        "first_doc = docs[0]\n",
        "tagged_sentences = ie_preprocess(first_doc)\n",
        "tagged_sentences[1]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('He', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('forward', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('serves', 'NNS'),\n",
              " ('as', 'IN'),\n",
              " ('captain', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('Portugal', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTwrZId_WU_"
      },
      "source": [
        "## Task 3 (20 Marks)\n",
        "Write a function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n",
        "\n",
        "Hint: Use `binary = True` while calling NE chunk function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fC0iqJJ_WU_"
      },
      "source": [
        "def find_named_entities(sent):\n",
        "  # Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on.\n",
        "  named_entities = []\n",
        "  # your code goes here\n",
        "  chunk = nltk.ne_chunk(sent, binary=True)  # nltk's named entity recognizer\n",
        "  \n",
        "  for subtree in chunk.subtrees():\n",
        "      if subtree.label() == 'NE':           # NE stands for named entity\n",
        "          entity = \"\"\n",
        "          for leaf in subtree.leaves():\n",
        "              entity = entity + leaf[0] + \" \"\n",
        "          named_entities.append(entity.strip())\n",
        "  return named_entities"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td5yJ8cgFScx"
      },
      "source": [
        "Run the cell below to verify your result for the first sentence in the first document.\n",
        "Expected output: `['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FijjdAPWFsp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf86f9bc-1bbb-455b-e3bf-eec3fae7c12b"
      },
      "source": [
        "tagged_sentences = ie_preprocess(docs[0])\n",
        "find_named_entities(tagged_sentences[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cristiano Ronaldo',\n",
              " 'Santos Aveiro',\n",
              " 'ComM',\n",
              " 'GOIH',\n",
              " 'Portuguese',\n",
              " 'Portuguese',\n",
              " 'Spanish',\n",
              " 'Real Madrid',\n",
              " 'Portugal']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHMp7xtK_WVE"
      },
      "source": [
        "## Task 4 (5 Marks)\n",
        "\n",
        "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n",
        "\n",
        "Hint: Use `find_named_entities` implemented above for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwFlzQx4_WVF"
      },
      "source": [
        "def find_all_named_entities(doc):\n",
        "    named_entities = []\n",
        "    # your code goes here\n",
        "    posTagSentences = ie_preprocess(doc)\n",
        "    for sentence in posTagSentences:\n",
        "        named_entity = find_named_entities(sentence)\n",
        "        if len(named_entity)!= 0:\n",
        "              named_entities.append(named_entity)  \n",
        "    # https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "    return [item for sublist in named_entities for item in sublist] # return a flatlist and not list of lists       "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdM0-LZlJy4u"
      },
      "source": [
        "How many named entities did you find in the first document?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ajmnnOqJ8V6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef06e41-0cff-4f64-94a0-3e385c1c1ec5"
      },
      "source": [
        "# your code goes here\n",
        "NE1 = find_all_named_entities(docs[0])\n",
        "answer = len(set(NE1))\n",
        "print(\"Number of unique named enities in first document : \",answer)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique named enities in first document :  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2AzD9MVNIx2"
      },
      "source": [
        "## Task 5 (5 Marks)\n",
        "\n",
        "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YULMcK1-NSR9"
      },
      "source": [
        "all_named_entities = []\n",
        "# your code goes here\n",
        "for doc in docs:\n",
        "  named_entity_doc = find_all_named_entities(doc) # named entity of single doc\n",
        "  all_named_entities.append(named_entity_doc)     # apeending named entities of each doc in single list\n",
        "\n",
        "# https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "all_named_entities = [item for sublist in all_named_entities for item in sublist] "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaM9Cs9zNGM2"
      },
      "source": [
        "How many named entities did you find across all documents?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCNIrC_SNpHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1863a9a6-bc2d-4b54-c994-61a1e864c5c1"
      },
      "source": [
        "# your code goes here\n",
        "answer_unique = len(set(all_named_entities))\n",
        "answer = len(all_named_entities)\n",
        "print(\"Number of times named enities existed across all documents : \",answer)\n",
        "print(\"Number of unique named enities across all documents : \",answer_unique)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of times named enities existed across all documents :  369\n",
            "Number of unique named enities across all documents :  150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7-ma9lJ_WVJ"
      },
      "source": [
        "## Task 6 (40 Marks)\n",
        "\n",
        "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
        "\n",
        "Hint: Use the `re.compile()` function to create the extraction patterns.\n",
        "\n",
        "Reference: https://docs.python.org/3/howto/regex.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5iX9UPyqneU"
      },
      "source": [
        "**Date of Birth of player**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZMsv5yZc25_",
        "outputId": "81dcbba8-0f80-4ea7-afe4-5d10c5f2fa9d"
      },
      "source": [
        "def date_of_birth(doc):    \n",
        "    #  born 5 February 1992), : We need to find something in this format\n",
        "    sentence = sent_tokenize(doc)[0]\n",
        "    # in regex + represent 1,2,3 ...  matching \n",
        "    # \\s Matches any whitespace character\n",
        "    # \\w Matches any alphanumeric character [a-zA-Z0-9_]\n",
        "    # \\d Matches any decimal digit [0-9]\n",
        "    # Reference : https://docs.python.org/3/howto/regex.html\n",
        "    reg = re.compile(r'born\\s\\d+\\s\\w+\\s\\d+') \n",
        "    dob = reg.findall(sentence)[0]\n",
        "    return dob[5:] # removing 'born '\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"DOB of player in doc {i} : \",date_of_birth(docs[i]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DOB of player in doc 0 :  5 February 1985\n",
            "DOB of player in doc 1 :  24 June 1987\n",
            "DOB of player in doc 2 :  5 February 1992\n",
            "DOB of player in doc 3 :  21 March 1980\n",
            "DOB of player in doc 4 :  24 October 1985\n",
            "DOB of player in doc 5 :  3 October 1981\n",
            "DOB of player in doc 6 :  2 May 1975\n",
            "DOB of player in doc 7 :  15 October 1988\n",
            "DOB of player in doc 8 :  16 July 1989\n",
            "DOB of player in doc 9 :  11 May 1984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-CNrMM5_WVO"
      },
      "source": [
        "Execute the cell below to verify the `date_of_birth` function for the third player. Expected output `5 February 1992`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpeKE1u9_WVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29eb293b-bacb-4694-861b-348650dbabe3"
      },
      "source": [
        "date_of_birth(docs[2])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'5 February 1992'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhleFefVq7nh"
      },
      "source": [
        "**Name of Player**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jjh3q7Pq5c4",
        "outputId": "b3b723ec-3ced-4eb3-bece-13564cc7b460"
      },
      "source": [
        "def name_of_the_player(doc):\n",
        "  # your code goes here\n",
        "  pos_sents = ie_preprocess(doc)\n",
        "  named_entities = []\n",
        "  # your code goes here\n",
        "  chunk = nltk.ne_chunk(pos_sents[0]) # nltk's named entity recognizer\n",
        "  fullName = None\n",
        "\n",
        "  for subtree in chunk.subtrees():\n",
        "      if subtree.label() == 'PERSON': # trying to find NE with person tag\n",
        "          entity = \"\"\n",
        "          for leaf in subtree.leaves():\n",
        "              entity = entity + leaf[0] + \" \"\n",
        "          named_entities.append(entity.strip())\n",
        "  if len(named_entities)!=0:\n",
        "    fullName = \" \".join(named_entities)\n",
        "  return fullName\n",
        "\n",
        "# Nltk is not able to find player's name in 6th document / Also sonetime not able to find full name of players\n",
        "for i in range(10):\n",
        "    print(f\"Name of player {i} : \",name_of_the_player(docs[i]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of player 0 :  Cristiano Santos Aveiro\n",
            "Name of player 1 :  Lionel\n",
            "Name of player 2 :  Silva Santos JÃºnior Neymar Neymar Jr.\n",
            "Name of player 3 :  Ronaldo Assis Moreira\n",
            "Name of player 4 :  Wayne Mark Rooney\n",
            "Name of player 5 :  None\n",
            "Name of player 6 :  David Robert Joseph Beckham\n",
            "Name of player 7 :  Mesut\n",
            "Name of player 8 :  Gareth Frank Bale\n",
            "Name of player 9 :  Iniesta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAOfjZtSq_fb"
      },
      "source": [
        "**Origin country of Player**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqSJttQxjrj9",
        "outputId": "1cbf19fb-abd1-444a-abc9-9e6795fe7344"
      },
      "source": [
        "def country_of_origin(doc):\n",
        "  # your code goes here\n",
        "  pos_sents = ie_preprocess(doc)\n",
        "  # https://sportytell.com/soccer/best-soccer-countries-world-rankings/\n",
        "  # These are top 20 country who play football, later new countries can be added accordingly\n",
        "  # This list is too check if our probable answer/location in in this list, it can be country of origin of that player\n",
        "  football_countries = ['Brazil','Argentina','France','Germany','England','Spain','Portugal','Uruguay','Belgium','Croatia'\n",
        "                        'Netherlands','Colombia','Italy','Mexico','Chile','Sweden','Switzerland','German','Denmark','Senegal','Algeria','Welsh']\n",
        "  named_entities = []\n",
        "  # your code goes here\n",
        "  chunk = nltk.ne_chunk(pos_sents[0]) # nltk's named entity recognizer\n",
        "  originCountry = None\n",
        "  \n",
        "  for subtree in chunk.subtrees():\n",
        "      if subtree.label() == 'GPE': # Trying to find location using nltk namd entities\n",
        "          entity = \"\"\n",
        "          for leaf in subtree.leaves():\n",
        "              entity = entity + leaf[0] + \" \"\n",
        "          named_entities.append(entity.strip())\n",
        "\n",
        "  # if anything found is in football_countries assign that as country  \n",
        "  for country in football_countries:\n",
        "      if country in named_entities:\n",
        "           originCountry = country\n",
        "      \n",
        "  # If we are still not able to find country - try searching near 'national team'\n",
        "  if originCountry == None:\n",
        "      # Reference : https://docs.python.org/3/howto/regex.html\n",
        "      match = re.compile(r'(\\w+)\\snational team')\n",
        "      sentences = sent_tokenize(doc)\n",
        "      for sent in sentences:\n",
        "          expected_country = match.findall(sent)\n",
        "          if len(expected_country)!=0:\n",
        "              country = expected_country[0].strip() # remove spaces\n",
        "              if country in football_countries:     # if anything found is in football_countries assign that as country\n",
        "                  originCountry = country\n",
        "  \n",
        "  # If we are still not able to find country - try searching near 'footballer'               \n",
        "  if originCountry == None:\n",
        "      # Reference : https://docs.python.org/3/howto/regex.html\n",
        "      match =  re.compile(r'(\\w+)\\sfootballer')\n",
        "      sentences = sent_tokenize(doc)\n",
        "      for sent in sentences:\n",
        "          expected_country = match.findall(sent)\n",
        "          if len(expected_country)!=0:\n",
        "              country = expected_country[0].strip() # remove spaces\n",
        "              if country in football_countries:     # if anything found is in football_countries assign that as country\n",
        "                  originCountry = country\n",
        "  # If we are still not able to find country - try searching near 'professional footballer'\n",
        "  if originCountry == None:\n",
        "      # Reference : https://docs.python.org/3/howto/regex.html\n",
        "      match =  re.compile(r'(\\w+)\\sprofessional\\sfootballer')\n",
        "      sentences = sent_tokenize(doc)\n",
        "      for sent in sentences:\n",
        "          expected_country = match.findall(sent)\n",
        "          if len(expected_country)!=0:\n",
        "              country = expected_country[0].strip() # remove spaces\n",
        "              if country in football_countries:     # if anything found is in football_countries assign that as country\n",
        "                  originCountry = country\n",
        "    \n",
        "  return originCountry\n",
        "  \n",
        " \n",
        "for i in range(10):\n",
        "    print(f\"Country of player {i} :\",country_of_origin(docs[i]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Country of player 0 : Portugal\n",
            "Country of player 1 : Argentina\n",
            "Country of player 2 : Brazil\n",
            "Country of player 3 : Brazil\n",
            "Country of player 4 : England\n",
            "Country of player 5 : Sweden\n",
            "Country of player 6 : England\n",
            "Country of player 7 : German\n",
            "Country of player 8 : Welsh\n",
            "Country of player 9 : Spain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCscpL25rFwL"
      },
      "source": [
        "**Team of player**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcbjItUFozf5",
        "outputId": "bf7668be-979e-4a3a-c968-16fe21239163"
      },
      "source": [
        "def team_of_the_player(doc):\n",
        "  # your code goes here\n",
        "  pos_sents = ie_preprocess(doc)\n",
        "  named_entities = []\n",
        "  chunk = nltk.ne_chunk(pos_sents[0]) # nltk's named entity recognizer\n",
        "  \n",
        "  # some football clubs , more can be added in future\n",
        "  football_clubs = ['Real Madrid','FC Barcelona','Manchester United','Arsenal','Liverpool','Juventus']\n",
        "  # https://sportytell.com/soccer/best-soccer-countries-world-rankings/\n",
        "  # These are top 20 country who play football, later new countries can be added accordingly\n",
        "  football_countries = ['Brazil','Argentina','France','Germany','England','Spain','Portugal','Uruguay','Belgium','Croatia'\n",
        "                        'Netherlands','Colombia','Italy','Mexico','Chile','Sweden','Switzerland','Denmark','Senegal','Algeria','Welsh','Wales']\n",
        "  \n",
        "  teams = str() # to store team of player\n",
        "  for subtree in chunk.subtrees():\n",
        "      if subtree.label() == 'ORGANIZATION': # Trying to find location using nltk named entities organization\n",
        "          entity = \"\"\n",
        "          for leaf in subtree.leaves():\n",
        "              entity = entity + leaf[0] + \" \"\n",
        "          named_entities.append(entity.strip())\n",
        "          \n",
        "  for club in football_clubs:\n",
        "      if club in named_entities:\n",
        "          teams = teams + str(club)\n",
        "    \n",
        "  # we might get some information near 'national team'  - A footballer can have two teams (National team and Club team)               \n",
        "  match = re.compile(r'(\\w+)\\snational team')\n",
        "  sentences = sent_tokenize(doc)\n",
        "  for sent in sentences:\n",
        "      expected_country = match.findall(sent)\n",
        "      if len(expected_country)!=0:\n",
        "          country = expected_country[0].strip() # remove spaces\n",
        "          if country in football_countries:     # if anything found is in football_countries assign that as country ( that will also his national team)\n",
        "              if len(teams)!=0:\n",
        "                  teams  = teams +\" and \"+str(country)+ \" National Team\"\n",
        "              else:\n",
        "                  teams = teams + str(country)+ \" National Team\"  \n",
        "  return teams\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Team of player {i} :\",team_of_the_player(docs[i]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Team of player 0 : Real Madrid and Portugal National Team\n",
            "Team of player 1 : FC Barcelona and Argentina National Team\n",
            "Team of player 2 : FC Barcelona and Brazil National Team\n",
            "Team of player 3 : FC Barcelona and Brazil National Team\n",
            "Team of player 4 : England National Team\n",
            "Team of player 5 : Manchester United and Sweden National Team\n",
            "Team of player 6 : England National Team\n",
            "Team of player 7 : Arsenal\n",
            "Team of player 8 : Real Madrid and Wales National Team\n",
            "Team of player 9 : FC Barcelona and Spain National Team\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GziNqJvkrNH0"
      },
      "source": [
        "**Position of Player in Football game**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqqvnMqEqZq9",
        "outputId": "7560f969-dbe1-4d6f-ccea-96dfcb0eaa24"
      },
      "source": [
        "def position_of_the_player(doc):\n",
        "  # your code goes here\n",
        "  sentences = sent_tokenize(doc)\n",
        "  # positions possible in the game of football\n",
        "  pos = [\"captain\", \"striker\", \"winger\", \"defensive tackle\", \"defensive end\",\"left forward\",\"right forward\",\n",
        "         \"full back\",\"center back\",\"sweeper\",\"coach\", \"attacking midfielder\",\"forward\", \"central midfielder\"]\n",
        "  player_position = []\n",
        "  for p in pos:\n",
        "      # search every position possible in football in our document using regex\n",
        "      for sent in sentences:\n",
        "          # https://stackoverflow.com/questions/500864/case-insensitive-regular-expression-without-re-compile\n",
        "          reg = re.compile(r'\\b({0})\\b'.format(p), re.IGNORECASE)    \n",
        "          if reg.search(sent) != None:\n",
        "              player_position.append(p)\n",
        "  uniquePosition = set(player_position)\n",
        "  return \" and \".join(list(uniquePosition))\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Position of player {i} :\",position_of_the_player(docs[i]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Position of player 0 : forward and captain\n",
            "Position of player 1 : forward and captain\n",
            "Position of player 2 : forward\n",
            "Position of player 3 : forward and attacking midfielder\n",
            "Position of player 4 : forward\n",
            "Position of player 5 : striker and captain\n",
            "Position of player 6 : winger and captain\n",
            "Position of player 7 : attacking midfielder\n",
            "Position of player 8 : winger\n",
            "Position of player 9 : central midfielder and captain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3VtWxBr_WVZ"
      },
      "source": [
        "## Task 6 (10 Marks)\n",
        "Identify one other relation (besides team and player) and write a function to extract it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR0GZrUB_WVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd58453d-6bea-4e7e-c1cb-a70b67731cdd"
      },
      "source": [
        "# your code goes here\n",
        "def awards_of_player(doc):\n",
        "    sentences = sent_tokenize(doc)\n",
        "    # Famous football awards source : Google (We can hardcode as the title of these award almost never changes)\n",
        "    footbal_awards = ['Ballon d\\'Or','FIFA World Player of the Year','European Golden Shoe'\n",
        "                      ,'UEFA Men\\'s Player of the Year','European Golden Shoe','FWA Footballer of the Year'\n",
        "                      ,'PFA Players\\' Player of the Year','Onze d\\'Or','FIFA Player of the Century'\n",
        "                      ,'FIFA World Coach of the Year','FIFA Presidential','FIFA Order of Merit','Globe Soccer'\n",
        "                      ,'Golden Foot','PSL Award','DZFoot d\\'Or','La Liga','FIFPro World 11','PFA Young Player']\n",
        "    player_awards = []\n",
        "    # if these words coming in sentences he has probably not won that award\n",
        "    stop_words = ['runner-up','finalist','nominations','finishing second','represented','appearing','nominated for']\n",
        "    for sent in sentences:\n",
        "        for award in footbal_awards:\n",
        "            X = [ item for item in stop_words if item in sent ] \n",
        "            if award in sent and len(X)==0:\n",
        "                player_awards.append(award)\n",
        "    return \" and \".join(list(set(player_awards)))\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Some notable awards of player {i}:\",awards_of_player(docs[i]))  "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some notable awards of player 0: Ballon d'Or and FIFA World Player of the Year and European Golden Shoe and La Liga\n",
            "Some notable awards of player 1: Ballon d'Or and FIFA World Player of the Year and European Golden Shoe and La Liga\n",
            "Some notable awards of player 2: Ballon d'Or and La Liga\n",
            "Some notable awards of player 3: Ballon d'Or and FIFA World Player of the Year and La Liga\n",
            "Some notable awards of player 4: FWA Footballer of the Year and Ballon d'Or and PFA Players' Player of the Year and FIFPro World 11\n",
            "Some notable awards of player 5: \n",
            "Some notable awards of player 6: La Liga\n",
            "Some notable awards of player 7: La Liga\n",
            "Some notable awards of player 8: FWA Footballer of the Year and PFA Players' Player of the Year and PFA Young Player\n",
            "Some notable awards of player 9: \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}